<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="论文笔记（一）">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记">
<meta property="og:url" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="chaofan">
<meta property="og:description" content="论文笔记（一）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-15%2016.38.20.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-15%2016.41.55.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/09/aT26XQ.png#shadow">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-15%2017.56.53.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/09/aTRe9f.jpg#shadow">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-15%2018.57.17.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-15%2019.04.51.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-16%2015.27.29.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-16%2015.28.46.png">
<meta property="og:image" content="https://s1.ax1x.com/2020/08/10/abpxoj.png#shadow">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-24%2011.18.56.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-03%2010.33.54.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-09%2010.54.23.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-09%2010.54.58.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-03%2011.01.26.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-03%2011.02.19.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-1cb89c617f2e954262571b032a4812fa_720w.webp">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-03%2014.11.12.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-03%2014.06.55.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-09%2010.45.36.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-09%2010.45.06.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-14%2017.07.10.png">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-cfd25e50699ff874c41b0cabf51a4ae0_720w.webp">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-05%2009.26.16.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-05%2010.20.47.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-05%2009.29.04.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-20%2010.35.53.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-20%2010.37.52.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-20%2010.43.37.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-20%2011.31.13.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-22%2016.32.55.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-27%2015.45.48.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-11-27%2016.13.28.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-12-01%2010.58.35.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-12-01%2011.08.47.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-12-01%2011.13.16.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-12-01%2011.22.09.png">
<meta property="og:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-12-01%2011.31.27.png">
<meta property="article:published_time" content="2022-11-26T15:37:56.000Z">
<meta property="article:modified_time" content="2022-12-07T03:13:04.877Z">
<meta property="article:author" content="chaofan">
<meta property="article:tag" content="学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/截屏2022-10-15%2016.38.20.png">

<link rel="canonical" href="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>论文笔记 | chaofan</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?283ce806723e4661e06c77a2910e4faa";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">chaofan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="chaofan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chaofan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          论文笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-26 23:37:56" itemprop="dateCreated datePublished" datetime="2022-11-26T23:37:56+08:00">2022-11-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-12-07 11:13:04" itemprop="dateModified" datetime="2022-12-07T11:13:04+08:00">2022-12-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-DL/" itemprop="url" rel="index"><span itemprop="name">ML/DL</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>论文笔记（一）<br><span id="more"></span></p>
<h1 id="Deep-Learning-for-Sentiment-Analysis-A-Survey"><a href="#Deep-Learning-for-Sentiment-Analysis-A-Survey" class="headerlink" title="Deep Learning for Sentiment Analysis: A Survey"></a><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1801.07883.pdf">Deep Learning for Sentiment Analysis: A Survey</a></h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>语义分析和意见挖掘是研究人们对待实体（entities）的 意见（opinions）, 情感（sentiments）, 情绪（emotions）, 评价（appraisals）和 态度（attitudes）</p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">有监督</th>
<th style="text-align:center">无监督</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SVM、Maximum Entropy、NB</td>
<td style="text-align:center">sentiment lexicons、grammatical analysis、syntactic patterns</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><div class="table-container">
<table>
<thead>
<tr>
<th>methods</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td>Word2Vec</td>
<td>Continuous Bag-of-Words model (CBOW), Skip-Gram model (SG)</td>
</tr>
<tr>
<td>Global Vector (GloVe)</td>
<td>trained on the nonzero entries of a global word-word co-occurrence matrix</td>
</tr>
<tr>
<td>BERT</td>
<td>SOTA</td>
</tr>
</tbody>
</table>
</div>
<h2 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h2><p>RNN/CNN/LSTM/Attention/Memory Network/RecNN</p>
<h2 id="Sentiment-Analysis-Tasks"><a href="#Sentiment-Analysis-Tasks" class="headerlink" title="Sentiment Analysis Tasks"></a>Sentiment Analysis Tasks</h2><p>由简到繁（对象的粒度）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层次</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td>document level</td>
<td>一整个文档对某个实体的意见评估：pos/neg</td>
</tr>
<tr>
<td>sentence level</td>
<td>subjectivity classification: 判断一句话是主观还是客观，若为主观，再判断pos/neg/neu</td>
</tr>
<tr>
<td>aspect level</td>
<td>判断一句话有几个层次（实体），对它们的情感aspect extraction, entity extraction, and aspect sentiment classification</td>
</tr>
</tbody>
</table>
</div>
<p>除此之外，情感分析还做emotion analysis, sarcasm detection, multilingual sentiment analysis这些方向。</p>
<h3 id="Document-Level"><a href="#Document-Level" class="headerlink" title="Document Level"></a>Document Level</h3><blockquote>
<p>Sentiment classification at the document level is to assign an overall sentiment orientation/polarity to an opinion document, i.e., To determine whether the <strong>document</strong> (e.g., a full online review) conveys an overall positive or negative opinion.</p>
</blockquote>
<p>document表示：BoW模型（忽略词序、语法、句法）→n-gram→dense vector，BERT？</p>
<p>网络模型：DNN、CNN、RNN、LSTM、Hierarchical Attention Network</p>
<p>推荐阅读：<a target="_blank" rel="noopener" href="https://cs.stanford.edu/~diyiy/docs/naacl16.pdf">HAN for Document Classification</a></p>
<h3 id="Sentence-Level"><a href="#Sentence-Level" class="headerlink" title="Sentence Level"></a>Sentence Level</h3><blockquote>
<p>Sentence level sentiment classification is to determine the sentiment expressed in a single given sentence.</p>
</blockquote>
<p>语法语义分析、解析树</p>
<h3 id="Aspect-Level"><a href="#Aspect-Level" class="headerlink" title="Aspect Level"></a>Aspect Level</h3><blockquote>
<p>Aspect level sentiment classification considers both the <strong>sentiment</strong> and the <strong>target</strong> information. A target is usually an entity or an entity aspect. For simplicity, both entity and aspect are usually just called aspect.</p>
<p>For example, in the sentence “the screen is very clear but the battery life is too short.” the sentiment is positive if the target aspect is “screen” but negative if the target aspect is “battery life”.</p>
</blockquote>
<p>三个重点：</p>
<ul>
<li>represent the context of a target. 生成目标对象上下文的表示：可以使用之前提到的文本表示方法</li>
<li>generate a target representation. 生成目标对象的表示：可以类似词嵌入，学习一个文本嵌入（target embedding）</li>
<li>identify the important sentiment context (words) for the specified target. 识别对于特定目标对象的重要情感上下文：目前常用注意力机制处理，但是还没有统治性的方法出现。</li>
</ul>
<h2 id="Aspect-extraction-amp-categorization"><a href="#Aspect-extraction-amp-categorization" class="headerlink" title="Aspect extraction &amp; categorization"></a>Aspect extraction &amp; categorization</h2><p>自动aspect（实体）抽取</p>
<p>可以建模为一个序列标注问题</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/waterdream19/article/details/107021471">有监督的评价对象抽取论文总结</a></p>
<h2 id="Opinion-expression-extraction"><a href="#Opinion-expression-extraction" class="headerlink" title="Opinion expression extraction"></a>Opinion expression extraction</h2><p>意见表达提取，旨在识别句子或文档中的情感表达</p>
<blockquote>
<p>identify the expressions of sentiment in a sentence or a document.</p>
</blockquote>
<h2 id="Sentiment-composition"><a href="#Sentiment-composition" class="headerlink" title="Sentiment composition"></a>Sentiment composition</h2><blockquote>
<p>Sentiment composition claims that the sentiment orientation of an opinion expression is determined by the meaning of its constituents as well as the grammatical structure.</p>
</blockquote>
<h2 id="Opinion-holder-extraction"><a href="#Opinion-holder-extraction" class="headerlink" title="Opinion holder extraction"></a>Opinion holder extraction</h2><p>意见发表者提取</p>
<h2 id="Temporal-opnion-mining"><a href="#Temporal-opnion-mining" class="headerlink" title="Temporal opnion mining"></a>Temporal opnion mining</h2><p>时态意见挖掘，预测未来观点。随着时间推移，观点会发生改变。</p>
<h2 id="Sentiment-analysis-with-word-embedding"><a href="#Sentiment-analysis-with-word-embedding" class="headerlink" title="Sentiment analysis with word embedding"></a>Sentiment analysis with word embedding</h2><p>词嵌入的情感分析。词嵌入在情感分析模型中起着重要作用。</p>
<h2 id="Sarcasm-analysis"><a href="#Sarcasm-analysis" class="headerlink" title="Sarcasm analysis"></a>Sarcasm analysis</h2><p>讽刺分析</p>
<p><a target="_blank" rel="noopener" href="https://paperswithcode.com/task/sarcasm-detection">sarcasm detection Benchmarks</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.02276.pdf">Sarcasm Detection: A Comparative Study</a></p>
<h2 id="Emotion-analysis"><a href="#Emotion-analysis" class="headerlink" title="Emotion analysis"></a>Emotion analysis</h2><p>情绪分析。主要的情绪包括爱、喜悦、惊讶、愤怒、悲伤和恐惧。情绪的概念与情感密切相关。</p>
<h2 id="Multimodal-data-for-sentiment-analysis"><a href="#Multimodal-data-for-sentiment-analysis" class="headerlink" title="Multimodal data for sentiment analysis"></a>Multimodal data for sentiment analysis</h2><p>多模态数据的情感分析。如承载文本、视觉和听觉信息的数据，已经被用来帮助情感分析，提供额外的情感信号。</p>
<p><a target="_blank" rel="noopener" href="https://paperswithcode.com/task/multimodal-sentiment-analysis">multimodal sentiment analysis Benchmarks</a></p>
<h2 id="Resource-poor-languages-and-multilingual-sentiment-analysis"><a href="#Resource-poor-languages-and-multilingual-sentiment-analysis" class="headerlink" title="Resource-poor languages and multilingual sentiment analysis"></a>Resource-poor languages and multilingual sentiment analysis</h2><p>资源贫乏语言与多语言情感分析</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/108168121">总结</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.07883?source=post_page---------------------------">论文arxiv主页</a></p>
<p><a target="_blank" rel="noopener" href="https://lalalei21.github.io/nlp/sentiment_analysis/#%E5%85%B6%E4%BB%96%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%9B%B8%E5%85%B3%E4%BB%BB%E5%8A%A1">情感分析论文</a></p>
<h1 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h1><p>从模型上来说，RoBERTa基本没有什么太大创新，主要是在BERT基础上做了几点调整：</p>
<ol>
<li><p>训练时间更长，batch size更大，训练数据更多；</p>
</li>
<li><p>移除了next predict loss；</p>
</li>
<li>训练序列更长；</li>
<li>动态调整Masking机制。</li>
<li>Byte level BPE RoBERTa is trained with dynamic masking (Section 4.1), </li>
<li>FULL - SENTENCES without NSP loss (Section 4.2), </li>
<li>large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4).</li>
</ol>
<h1 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h1><h2 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h2><ul>
<li>提出了新的模型预训练的框架，采用generator和discriminator的结合方式，但又不同于GAN</li>
<li>将Masked Language Model的方式改为了replaced token detection</li>
<li>因为masked language model 能有效地学习到context的信息，所以能很好地学习embedding，所以使用了weight sharing的方式将generator的embedding的信息共享给discriminator</li>
<li>dicriminator 预测了generator输出的每个token是不是original的，从而高效地更新transformer的各个参数，使得模型的熟练速度加快</li>
<li>该模型采用了小的generator以及discriminator的方式共同训练，并且采用了两者loss相加，使得discriminator的学习难度逐渐地提升，学习到更难的token（plausible tokens）</li>
<li>模型在fine-tuning 的时候，丢弃generator，只使用discriminator</li>
</ul>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/133233364">ELECTRA论文阅读笔记</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/118135466">ELECTRA 详解</a></p>
<h1 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h1><p>BERT方法的预训练和微调过程的差异性：预训练时输入的部分词被mask，而微调时并不存在mask词。XLNet优点：</p>
<ol>
<li>通过最大化因式分解顺序的所有排列的预期可能性来学习双向语境</li>
<li>自回归，使得输入可以不限长度</li>
<li>融合Transformer-XL思路</li>
</ol>
<h2 id="AR-or-AE"><a href="#AR-or-AE" class="headerlink" title="AR or AE ?"></a>AR or AE ?</h2><p>AR(Auto-regressive)</p>
<ul>
<li>前一个词的输出作为预测这个词的输入</li>
<li>无法获得双向语境信息</li>
<li>ELMo（双向的AR）、GPT</li>
<li>给定文本序列$\bold{x}=[x_1, …x_T]$，目标是调整参数使得训练数据上的似然函数最大：<br><img src="./截屏2022-10-15 16.38.20.png" style="zoom:80%"></li>
</ul>
<p>AE(Auto-encoding)</p>
<ul>
<li>即：mask方法，可使用双向的信息</li>
<li>[mask] 信息没有被使用</li>
<li>Fine-tuning阶段看不到这种被强行加入的 [mask] 标记，两个阶段存在使用模式不一致的情形，可能带来一定的性能损失</li>
<li>BERT 通过将序列 $\bold{x}$ 中随机挑选 15% 的 Token 变成 [MASK] 得到带噪声版本的 $\hat{\bold{x}}$。假设被 Mask 的原始值为$\bar{\bold{x}}$，那么 BERT 希望尽量根据上下文恢复（猜测）出原始值，也就是<br><img src="./截屏2022-10-15 16.41.55.png" style="zoom:80%"><br>表示$t$位置是一个[mask]，即只需要求 [mask] 掉位置的概率，$x’$ 代表除 $x$ 外的序列中所有词。</li>
</ul>
<p><strong>XLNet出发点：融合两者的优点，具体来说，站在AR的角度，引入和双向语言模型等价的效果。使得模型看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息</strong></p>
<h2 id="Permutation-language-model"><a href="#Permutation-language-model" class="headerlink" title="Permutation language model"></a>Permutation language model</h2><p>具体实现方式：<strong>通过随机取一句话的随机排列的一种，然后将末尾一定量的词给 “遮掩”（和 BERT 里的直接替换 “[MASK]” 有些不同）掉，最后用 AR 的方式来按照这种排列方式依此预测被 “遮掩” 掉的词</strong>。目标不是具体要预测哪个词，而是<strong>谁在最后，就预测谁</strong>。</p>
<p><img src="https://s1.ax1x.com/2020/08/09/aT26XQ.png#shadow" alt="img" style="zoom:67%;" /></p>
<p>挑选最后的几个做mask呢？设置超参数K，K等于总长度除以需要预测的个数，如：K=7/2，论文中给出实验最佳K值介于6和7之间，取倒数：$(\frac{1}{7}, \frac{1}{6})$，而BERT中的15%恰好处于之间。</p>
<p>对于一个长度为T的句子，为节省计算量，只随机采样 $T!$ 里的部分排列，$Z_T$ 表示长度为 T 的序列的所有排列的集合，$z\in Z_T$ 是其中一种排列，$z_t$ 表示排列的第 $t$ 个元素，而 $z_{&lt;t}$ 表示 $z$ 的第 1 到 t-1 个元素</p>
<p>Permutation LM的目标使似然概率最大（从所有的排列中采样一种，然后根据这个排列来分解联合概率成条件概率的乘积，然后加起来）</p>
<p><img src="./截屏2022-10-15 17.56.53.png" alt="截屏2022-10-15 17.56.53" style="zoom: 67%;" /></p>
<h3 id="如何Permute？"><a href="#如何Permute？" class="headerlink" title="如何Permute？"></a>如何Permute？</h3><p>Permutation 具体的实现方式不是打乱输入句子的顺序（在fine-tuning阶段也不可能实现），而是通过对 Transformer 的 <strong>Attention Mask</strong> 进行操作</p>
<p><img src="https://s1.ax1x.com/2020/08/09/aTRe9f.jpg#shadow" alt="img" style="zoom:50%;" /></p>
<p>比如序号为1234的句子，随机取一种排列3241，Attention mask如上图，第 $e_{ij}$ 为红色代表打乱后的排序下第 $i$ 个元素能知道第 $j$ 个元素的信息，白色为不知道。</p>
<p>具体的，在 Transformer 内部，通过 Attention 掩码，从 $\bold{x}$ 的输入单词里面，也就是 $x_i$ 的上文和下文单词中，随机选择 $i-1$ 个，放到 $x_i$ 的上文位置中，把其它单词的输入通过 Attention 掩码隐藏掉，于是就能够达成我们期望的目标（当然这个所谓放到 $x_i$ 的上文位置，只是一种形象的说法，其实在内部，就是通过 Attention Mask ，把其它没有被选到的单词 Mask 掉，不让它们在预测单词 $x_i$ 的时候发生作用而已。看着就类似于把这些被选中的单词放到了上文 Context_before 的位置了）。</p>
<h3 id="位置信息？"><a href="#位置信息？" class="headerlink" title="位置信息？"></a>位置信息？</h3><p>直接使用这个方法会产生问题：</p>
<p>假设输入的句子是”I like New York”，并且一种排列为 z=[1, 3, 4, 2]，假设我们需要预测的是 $z_3=4$ 根据公式：</p>
<p>$p_\theta(X_{z_3}=x|x_{z_1z_2})=p_\theta(X_4=x|x_1x_3)=\dfrac{\mathrm{exp}(e(x)^Th_\theta (x_1x_3))}{\sum_{x’}\mathrm{exp}(e(x’)^Th_\theta(x_1x_3))}$</p>
<p>式中 $x$ = York，$z_i$ 代表打乱排列后的第 i 个词，$x_i$ 代表原本序列中的第 i 个词。另外我们再假设一种排列为 z’=[1,3,2,4]，我们需要预测 $z_3=2$ 根据公式：</p>
<p>$p_\theta(X_{z_3}=x|x_{z_1z_2})=p_\theta(X_2=x|x_1x_3)=\dfrac{\mathrm{exp}(e(x)^Th_\theta (x_1x_3))}{\sum_{x’}\mathrm{exp}(e(x’)^Th_\theta(x_1x_3))}$</p>
<p>可以看出，二者的结果是一样的，即问题出在模型并不知道要预测的那个词在原始序列中的位置。（注意 Transformer 输入了位置编码，但位置编码是和输入的 Embedding 加到一起作为输入的，即 $x_1, x_3$ 带有正确的位置信息，但对于需要预测的 $z_3$ ，模型是不知道位置信息的）所以需要“显式”地告诉模型需要预测的原始位置信息。</p>
<p>$p_\theta(X_{z_t}=x|\bold{x}_{z&lt;t})=\dfrac{\mathrm{exp}(e(x)^T g_\theta (\bold{x}_{z_{&lt;t}},z_t))}{\sum_{x’}\mathrm{exp}(e(x’)^T g_\theta (\bold{x}_{z_{&lt;t}},z_t))}$</p>
<p>上式中的 $g_\theta (\bold{x}_{z_{&lt;t}},z_t)$ 表示一个包含词 $\bold{x}_{z_{&lt;t}}$ 和位置信息 $z_t$ 的新模型。</p>
<h3 id="Two-Stream-Self-Attention"><a href="#Two-Stream-Self-Attention" class="headerlink" title="Two-Stream Self-Attention"></a>Two-Stream Self-Attention</h3><p>如何表示 $g_\theta$ ，需要满足两点要求：</p>
<ul>
<li>预测 $\bold{x}_{z_t}$，$g_\theta$ 只能使用位置信息 $z_t$ 而不能直接使用 $\bold{x}_{z_t}$ </li>
<li>为了预测 $z_t$ 之后的词，$g_\theta$ 必须编码 $\bold{x}_{z_t}$ 的信息</li>
</ul>
<p>解决方法：使用包含两个隐状态的模型</p>
<ol>
<li>内容隐状态 content stream: $h_\theta(\bold{x}_{z_{\leq t}})$ ，和标准的Transformer一样，既编码上下文，也编码 $x_{z_t}$ 本身。</li>
<li>查询隐状态 query stream: $g_\theta(\bold{x}_{z_{&lt;t}},z_t)$，只编码上下文和要预测的位置 $z_t$，但是不包括 $x_{z_t}$ 本身。用于代替Bert中的 [mask] 标记。</li>
</ol>
<p>计算过程，从 $m=1$ 到第 $M$ 层逐层计算：</p>
<p><img src="./截屏2022-10-15 18.57.17.png" style="zoom:80%"></p>
<p>梯度更新和标准的Self-Attention一样，<strong>在fine-tune的时候可以丢掉query流只使用content流</strong>。（二者权重共享？）</p>
<p>模型的大致结构：</p>
<p><img src="./截屏2022-10-15 19.04.51.png" style="zoom:80%"></p>
<p>假设排列为3-2-4-1，并且现在预测第 1 个位置（原排列的位置1，重组排列后的位置4）的词的概率。</p>
<ul>
<li><p>图a中是Content流的计算，可以参考所有4个词的Content，因此$K\&amp;V=[h_1^{(0)},h_2^{(0)},h_3^{(0)},h_4^{(0)}]$，$Q=h_1^{(0)}$，其实就是标准的Transformer的计算过程</p>
</li>
<li><p>图b中是Query流的计算，不能参考自己的内容，因此$K\&amp;V=[h_1^{(0)},h_2^{(0)},h_3^{(0)}]$，$Q=g_1^{(0)}$</p>
</li>
<li>图c中是完整计算过程，首先 $h$ 和 $g$ 分别被初始化为 $e(x_i)$ 和 $W$，然后分别计算得到第一层的输出，注意右边的 attention mask 区别</li>
</ul>
<p>for content stream：</p>
<p><img src="./截屏2022-10-16 15.27.29.png" alt="截屏2022-10-16 15.27.29" style="zoom:80%;" /></p>
<p>for query stream: </p>
<p><img src="./截屏2022-10-16 15.28.46.png" alt="截屏2022-10-16 15.28.46" style="zoom:80%;" /></p>
<h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860">Transformer-XL</a></h2><h3 id="Segment-Recurrence-Mechanism-段循环机制"><a href="#Segment-Recurrence-Mechanism-段循环机制" class="headerlink" title="Segment Recurrence Mechanism 段循环机制"></a>Segment Recurrence Mechanism 段循环机制</h3><p>简单介绍一下：解决了Transformer不能一次性输入太长文本的问题。可以理解为Transformer+RNN</p>
<p><img src="https://s1.ax1x.com/2020/08/10/abpxoj.png#shadow" alt="img" style="zoom:67%;" /></p>
<p>对长序列进行分段，在前一段计算完后，将它计算出的隐状态都保存下来，存到一个 Memeory 中，之后在计算当前段的时候，将之前存下来的隐状态和当前段的隐状态拼起来，作为 Attention 机制的 K 和 V，从而获得更长的上下文信息</p>
<h3 id="Relative-Position-Encoding-相对位置编码"><a href="#Relative-Position-Encoding-相对位置编码" class="headerlink" title="Relative Position Encoding 相对位置编码"></a>Relative Position Encoding 相对位置编码</h3><ul>
<li>Transformer中考虑了序列的位置信息，在分段的情况下，如果仅仅对于每个段仍直接使用 Transformer 中的位置编码，即每个不同段在同一个位置上的表示使用相同的位置编码，就会出现问题。比如，第 i−2 段和第 i−1 段的第一个位置将具有相同的位置编码，但它们对于第 i 段重要性显然并不相同</li>
<li><strong>相对位置编码</strong>，不再关心句中词的绝对位置信息，而是相对的，比如说两个词之间隔了多少个词这样的相对信息</li>
</ul>
<h2 id="Relative-Segment-Encoding-相对段编码？"><a href="#Relative-Segment-Encoding-相对段编码？" class="headerlink" title="Relative Segment Encoding 相对段编码？"></a>Relative Segment Encoding 相对段编码？</h2><ul>
<li>改进 Bert 中的 NSP 任务</li>
<li>和 BERT 一样，选择两个句子，它们有 50% 的概率是连续的句子（前后语义相关），有 50% 的概率是不连续（无关) 的句子。把这两个句子拼接后当成一个句子来学习 Permutation LM。输入和 BERT 是类似的：[A, SEP, B, SEP, CLS]。</li>
<li>BERT 使用的是绝对的 Segment 编码，也就是第一个句子对于的 Segment id 是 0，而第二个句子是 1。这样如果把两个句子换一下顺序，那么输出是不一样的。XLNet 使用的是相对的 Segment 编码，它是在计算 Attention 的时候判断两个词是否属于同一个 Segment，如果位置 i 和 j 的词属于同一个 segment，那么使用一个可以学习的 Embedding $s_{ij}=s+$，否则 $s_{ij}=s−$，也就是说，我们只关心它们是属于同一个 segment 还是属于不同的 segment。</li>
<li>计算 attention 时：$a_{ij}=(q_i+b)^T s_{ij}$，其中 $q_i$ 是第 $i$ 个位置的 query 向量。</li>
<li>最后我们会把这个 attention score 加到原来计算的 attention weight 里，这样它就能学到当 i 和 j 都属于某个 segment 的特征，以及 i 和 j 属于不同 segment 的特征</li>
</ul>
<h2 id="XLNet-与-Bert-对比"><a href="#XLNet-与-Bert-对比" class="headerlink" title="XLNet 与 Bert 对比"></a>XLNet 与 Bert 对比</h2><p>假设输入是 [New, York, is, a, city] ，并且假设恰巧XLNet和BERT都选择使用 [is, a, city] 来预测 ‘New’ 和 ‘York’。同时我们假设XLNet的排列顺序为 [is, a, city, New, York] 。那么它们优化的目标函数分别为：</p>
<p>$\jmath_{\mathrm{BERT}} = \mathrm{log} p(\mathrm{New}|\mathrm{is\ a\ city}) + \mathrm{log} p(\mathrm{York}|\mathrm{is\ a\ city})$</p>
<p>$\jmath_{\mathrm{XLNet}} = \mathrm{log} p(\mathrm{New}|\mathrm{is\ a\ city}) + \mathrm{log} p(\mathrm{York}|\mathrm{is\ a\ city\ New})$</p>
<p>可以发现，XLNet可以在预测York的使用利用New的信息，因此它能学到”New York”经常出现在一起而且它们出现在一起的语义和单独出现是完全不同的。</p>
<h2 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h2><h3 id="data-utils-py"><a href="#data-utils-py" class="headerlink" title="data_utils.py"></a>data_utils.py</h3><p>Sentence piece 模型</p>
<p>读取每一个文件的每一行，然后使用sp切分成WordPiece，然后变成id，放到数组input_data里。另外还有一个sent_ids，用来表示句子。对于每一个文件(我们这里只有一个)，最终是为了得到”input_data, sent_ids = [], []”两个list。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_data&#x3D;[19  5372 19317  6014   111 17634  2611    19  1084   111   420   152    25  6096    26  8888]</span><br><span class="line">sent_ids&#x3D;[ True  True  True  True  True  True  True False False False False False False False False False]</span><br></pre></td></tr></table></figure>
<p>第一个句子是”我的空调漏水怎么办”，使用sp切分后变成”[‘▁’, ‘我的’, ‘空调’, ‘漏’, ‘水’, ‘怎么’, ‘办’]”，最后变成ID得到[19, 5372, 19317, 6014, 111, 17634, 2611]。而sent_ids是[ True  True  True  True  True  True  True]。</p>
<p>sent_ids可以判断哪些ID是属于一个句子的，也就是sent_ids通过交替的True和False来告诉我们句子的边界</p>
<p>拼接句子需要保证反转句子的最后一个 sent_id 与下一句第一个不同。</p>
<p>生成 Pretrain 数据：{ 64(reuse_len) A [sep] B [sep] [CLS] } 共128。</p>
<p>固定64个作为cache。然后从i+reuse_len位置开始不断寻找句子，直到这些句子的Token数大于61(128-64-3)。</p>
<h2 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h2><p>论文解读：</p>
<p><a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1475/">https://wmathor.com/index.php/archives/1475/</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/110204573">https://zhuanlan.zhihu.com/p/110204573</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70257427">https://zhuanlan.zhihu.com/p/70257427</a></p>
<p>代码解读：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37947156/article/details/100315836">https://blog.csdn.net/weixin_37947156/article/details/100315836</a></p>
<h1 id="BERT-CHN-WWM"><a href="#BERT-CHN-WWM" class="headerlink" title="BERT-CHN-WWM"></a>BERT-CHN-WWM</h1><h2 id="wwm-whole-word-masking-or-N-gram-mask"><a href="#wwm-whole-word-masking-or-N-gram-mask" class="headerlink" title="wwm(whole word masking) or N-gram mask?"></a>wwm(whole word masking) or N-gram mask?</h2><p><img src="./截屏2022-10-24%2011.18.56.png" alt="截屏2022-10-24 11.18.56" style="zoom:80%;" /></p>
<ul>
<li>wwm: 虽然 token 是最小的单位，但在 [MASK] 的时候是基于分词的</li>
<li>N-gram Masking: 是对连续n个词进行 [MASK]，如图中把“语 言 模 型”都 [MASK 了，就是一个 2-gram Masking。<strong>虽然[MASK]是对分词后的结果进行，但在输入的时候还是单个的token。</strong></li>
<li>MacBERT: 采用基于分词的 n-gram masking，1-gram~4-gram Masking的概率分别是40%、30%、20%、10%。</li>
</ul>
<h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><h3 id="Mac-MLM-as-correction"><a href="#Mac-MLM-as-correction" class="headerlink" title="Mac(MLM as correction)"></a>Mac(MLM as correction)</h3><ul>
<li>使用全词掩码和N-gram掩码</li>
<li>取消 [mask] token，而使用随机近义词替换，如果没有近义词，使用随机替换</li>
<li>15% input，80% 替换为近义词，10% 随机替换，10% 不替换</li>
</ul>
<h3 id="SOP-sentence-order-prediction"><a href="#SOP-sentence-order-prediction" class="headerlink" title="SOP(sentence order prediction)"></a>SOP(sentence order prediction)</h3><p>结构和NSP一样，两个句子是连续的为正例，负例为交换原文两句子顺序</p>
<h1 id="ERNIE-THU"><a href="#ERNIE-THU" class="headerlink" title="ERNIE-THU"></a>ERNIE-THU</h1><p>与K-BERT的不同是，ERNIE的token和entity用的两套注意力机制参数，然后合并时再用一套参数，而K-BERT是直接把知识插入到原来的句子里，只使用一套注意力机制的参数。</p>
<p><img src="./截屏2022-11-03 10.33.54.png" alt="截屏2022-11-03 10.33.54"  /></p>
<p><img src="./截屏2022-11-09 10.54.23.png" alt="截屏2022-11-09 10.54.23" style="zoom:67%;" /></p>
<p><img src="./截屏2022-11-09 10.54.58.png" style="zoom:80%"></p>
<h2 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h2><p>T-Encoder单独使用了token-input，和BERT一样: $\{\boldsymbol{w_1,…,w_n}\}=\mathrm{T-Encoder}(\{w_1,…,w_n\})$ 。</p>
<p>K-Encoder使用了T-encoder处理后的token-input和TransE处理后的entity embedding: $\{\boldsymbol{w^o_1,…,w^o_n}\},\{\boldsymbol{e^o_1,…,e^o_m}\}=\mathrm{K-Encoder}(\{\boldsymbol{w_1,…,w_n}\},\{\boldsymbol{e_1,…,e_m}\})$</p>
<p>在K-Enocder中，token-input和entity-input使用的是两套不同参数的mh-att: </p>
<p>$\{\boldsymbol{\tilde{w_j}^{(i)}}\}=\mathrm{MH-ATT}_1(\{\boldsymbol{w_j^{(i-1)}}\}),$</p>
<p>$\{\boldsymbol{\tilde{e_k}^{(i)}}\}=\mathrm{MH-ATT}_2(\{\boldsymbol{e_k^{(i-1)}}\})$</p>
<p>size: $N = 6, M = 6, H_w = 768,H_e = 100,A_w = 12,A_e = 4$</p>
<p>参数量：114M (BERT-base: 12-layer, 768-hidden, 12-heads, 110M) </p>
<blockquote>
<p>knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance.</p>
</blockquote>
<h3 id="information-fusion"><a href="#information-fusion" class="headerlink" title="information fusion"></a>information fusion</h3><p>对于一个有对应实体的token：</p>
<p><img src="./截屏2022-11-03 11.01.26.png" style="zoom:80%"></p>
<p>对于没有对应实体的token：</p>
<p><img src="./截屏2022-11-03 11.02.19.png" style="zoom:80%"></p>
<p>作为下一个K-encoder的输入</p>
<p>Size of $h_j$: <code>intermediate_size=3072</code></p>
<h2 id="TransE"><a href="#TransE" class="headerlink" title="TransE"></a>TransE</h2><blockquote>
<p>Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE.</p>
</blockquote>
<p>entity使用TransE的嵌入表示然后再输入到K-Encoder</p>
<p><img src="https://pic3.zhimg.com/80/v2-1cb89c617f2e954262571b032a4812fa_720w.webp" alt="v2-1cb89c617f2e954262571b032a4812fa_720w"></p>
<p>ERNIE中的entity注入方式是通过注入entity的<strong>embedding（嵌入）</strong>来隐含所有关于entity的信息</p>
<h2 id="预训练方式-dEA"><a href="#预训练方式-dEA" class="headerlink" title="预训练方式 dEA"></a>预训练方式 dEA</h2><p>denoising entity auto-encoder (dEA)</p>
<p>随机mask一些 token-entity alignments（词-实体的组合），模型根据 aligned tokens $w_i$ 去预测entities $e_k$。</p>
<p><img src="./截屏2022-11-03 14.11.12.png" alt="截屏2022-11-03 14.11.12" style="zoom:67%;" /></p>
<ul>
<li>5% 对于一个token-entity alignment，随机替换entity，让模型预测正确的entity</li>
<li>15% 随机mask掉 token-entity alignments，让模型正确预测token-entity alignment</li>
<li>剩下的时间不改变</li>
</ul>
<p>cross-entropy loss function</p>
<p>MLM和NSP任务同时进行，总损失是dEA, MLM, NSP的损失之和</p>
<h2 id="Fine-tuning-for-specific-tasks"><a href="#Fine-tuning-for-specific-tasks" class="headerlink" title="Fine-tuning for specific tasks"></a>Fine-tuning for specific tasks</h2><p><img src="./截屏2022-11-03 14.06.55.png" style="zoom:80%"></p>
<p>对于普通的分类任务，采用的仍然是[CLS]作为输出，对一些knowledge driven的任务，比如：</p>
<ul>
<li>relation classificaiton 增加 [ENT] token</li>
<li>entity typing 增加 [HD] [TL] token</li>
</ul>
<h2 id="GLUE-Result"><a href="#GLUE-Result" class="headerlink" title="GLUE Result"></a>GLUE Result</h2><p><img src="./截屏2022-11-09 10.45.36.png" alt="截屏2022-11-09 10.45.36" style="zoom:80%;" /></p>
<h2 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h2><p>On FewRel (%)</p>
<p><img src="./截屏2022-11-09 10.45.06.png" alt="截屏2022-11-09 10.45.06" style="zoom:80%;" /></p>
<h2 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h2><p>论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.07129">https://arxiv.org/abs/1905.07129</a></p>
<p>代码 <a target="_blank" rel="noopener" href="https://github.com/thunlp/ERNIE">https://github.com/thunlp/ERNIE</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-05-26-4">https://www.jiqizhixin.com/articles/2019-05-26-4</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103208601">https://zhuanlan.zhihu.com/p/103208601</a></p>
<p>百度/清华 ERNIE <a target="_blank" rel="noopener" href="https://blog.csdn.net/LoseInVain/article/details/113859683">https://blog.csdn.net/LoseInVain/article/details/113859683</a></p>
<h1 id="ERNIE-Baidu-1-0"><a href="#ERNIE-Baidu-1-0" class="headerlink" title="ERNIE-Baidu 1.0"></a>ERNIE-Baidu 1.0</h1><p>ERNIE Github: <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></p>
<h2 id="Knowledge-Masking"><a href="#Knowledge-Masking" class="headerlink" title="Knowledge Masking"></a>Knowledge Masking</h2><p>模型在预测未知词的时候，没有考虑到外部知识。但是如果我们在mask的时候，加入了外部的知识，模型可以获得更可靠的语言表示。</p>
<blockquote>
<p>例如： 哈利波特是J.K.罗琳写的小说。 单独预测 <code>哈[MASK]波特</code> 或者 <code>J.K.[MASK]琳</code> 对于模型都很简单，但是模型不能学到<code>哈利波特</code>和<code>J.K. 罗琳</code>的关系。如果把<code>哈利波特</code>直接MASK掉的话，那模型可以根据作者，就预测到小说这个实体，实现了知识的学习。</p>
</blockquote>
<p>需要注意的是这些知识的学习是在训练中隐性地学习，而不是直接将外部知识的embedding加入到模型结构中（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.07129.pdf">ERNIE-TsingHua</a>的做法），模型在训练中学习到了更长的语义联系，例如说实体类别，实体关系等，这些都使得模型可以学习到更好的语言表达。</p>
<p>ERNIE的mask的策略是通过三个阶段学习的，在第一个阶段，采用的是BERT的模式basic-level masking，然后在加入词组的mask(phrase-level masking)，通过一些语义分割工具判断短语和实体；然后在加入实体级别entity-level的mask。</p>
<p><img src="./截屏2022-11-14 17.07.10.png" style="zoom:80%"></p>
<h2 id="更多的语料信息"><a href="#更多的语料信息" class="headerlink" title="更多的语料信息"></a>更多的语料信息</h2><p>包括一些对话的数据</p>
<h1 id="ERNIE-Baidu-2-0"><a href="#ERNIE-Baidu-2-0" class="headerlink" title="ERNIE-Baidu 2.0"></a>ERNIE-Baidu 2.0</h1><h2 id="Continual-Pre-training"><a href="#Continual-Pre-training" class="headerlink" title="Continual Pre-training"></a>Continual Pre-training</h2><ul>
<li>初始化 optimized initialization 每次有新任务过来，持续学习的框架使用的之前学习到的模型参数作为初始化，然后将新的任务和旧的任务一起训练。</li>
<li>训练任务安排 task allocating 对于多个任务，框架将自动的为每个任务在模型训练的不同阶段安排N个训练轮次，这样保证了有效率地学习到多任务。如何高效的训练，每个task 都分配有N个训练iteration。 One left problem is how to make it trained more efﬁciently. We solve this problem by allocating each task N training iterations. Our framework needs to automatically assign these N iterations for each task to different stages of training. In this way, we can guarantee the efﬁciency of our method without forgetting the previously trained knowledge</li>
<li>部分任务的语义信息建模适合递进式</li>
<li>比如ernie1.0 突破完形填空</li>
<li>ernie2.0 突破选择题，句子排序题等</li>
<li>不断递进更新，就好像是前面的任务都是打基础，有点boosting的意味</li>
<li>顺序学习容易导致遗忘模式（这个可以复习一下李宏毅的视频），所以只适合学习任务之间比较紧密的任务，就好像你今天学了JAVA，明天学了Spring框架，但是如果后天让你学习有机化学，就前后不能够联系起来，之前的知识就忘得快</li>
<li>适合递进式的语音建模任务： MLM word -&gt; whole word -&gt; name entity</li>
</ul>
<h2 id="Pre-training-tasks"><a href="#Pre-training-tasks" class="headerlink" title="Pre-training tasks"></a>Pre-training tasks</h2><p>ERNIE模型堆叠了大量的预训练目标。</p>
<ul>
<li><p>词法层级的任务(word-aware pretraining task)：获取词法知识。</p>
</li>
<li><p>knowledge masking(1.0) ERNIE1.0的任务</p>
</li>
<li>大小写预测（Capitalization Prediction Task） 模型预测一个字不是不是大小写，这个对特定的任务例如NER比较有用。（但是对于中文的话，这个任务比较没有用处，可能可以改为预测某个词是不是缩写）</li>
<li>词频关系（Token-Document Relation Prediction Task） 预测一个词是不是会多次出现在文章中，或者说这个词是不是关键词。</li>
<li>语法层级的任务(structure-aware pretraining task) ：获取句法的知识</li>
<li>句子排序(Sentence Reordering Task) 把一篇文章随机分为i = 1到m份，对于每种分法都有 i! 种组合，所以总共有$\sum_{i=i}^m i!$种组合，让模型去预测这篇文章是第几种，就是一个多分类的问题。这个问题就能够让模型学到句子之间的顺序关系。就有点类似于Albert的SOP任务的升级版。</li>
<li>句子距离预测(Sentence Distance Task) 一个三分类的问题：<ul>
<li>0: 代表两个句子相邻</li>
<li>1: 代表两个句子在同个文章但不相邻</li>
<li>2: 代表两个句子在不同的文章中</li>
</ul>
</li>
<li>语义层级的任务(semantic-aware pretraining task) ：获取语义关系的知识</li>
<li>篇章句间关系任务(Discourse Relation Task) 判断句子的语义关系例如logical relationship( is a, has a, contract etc.)</li>
<li>信息检索关系任务(IR Relevance Task) 一个三分类的问题，预测query和网页标题的关系<ul>
<li>0: 代表了提问和标题强相关（出现在搜索的界面且用户点击了）</li>
<li>1: 代表了提问和标题弱相关（出现在搜索的界面但用户没点击）</li>
<li>2: 代表了提问和标题不相关（未出现在搜索的界面）</li>
</ul>
</li>
</ul>
<h2 id="Continual-Fine-tuning"><a href="#Continual-Fine-tuning" class="headerlink" title="Continual Fine-tuning"></a>Continual Fine-tuning</h2><p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103190005">https://zhuanlan.zhihu.com/p/103190005</a></p>
<h1 id="Trans家族模型"><a href="#Trans家族模型" class="headerlink" title="Trans家族模型"></a>Trans家族模型</h1><h2 id="TransE-1"><a href="#TransE-1" class="headerlink" title="TransE"></a><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html">TransE</a></h2><p>希望学出一个简单的模型，将实体和实体之间的关系表示成向量。</p>
<p>TransE，通过将关系表示为在embedding空间中的平移来建模。也就是把头实体和尾实体都表示成向量，然后用“头实体+关系=尾实体”去优化损失。</p>
<p><img src="https://pic1.zhimg.com/80/v2-cfd25e50699ff874c41b0cabf51a4ae0_720w.webp" alt="v2-cfd25e50699ff874c41b0cabf51a4ae0_720w" style="zoom: 67%;" /></p>
<p>对于一个三元组 (h,l,t)，其中h是头实体，t是尾实体，l是它们的关系，TransE希望它们的嵌入表示 (h,l,t)有如下关系： t≈h+l，也就是说t要和h+l尽可能接近。反之，如果这三者不构成三元组，则要尽可能远离。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>Hinge loss:</p>
<p><img src="./截屏2022-11-05 09.26.16.png" alt="截屏2022-11-05 09.26.16" style="zoom:67%;" /></p>
<p>S为正例三元组，S’为负例，S‘的构建是随机替换正例三元组中的h或者t得到。d是l1或l2距离。</p>
<p>梯度（以L2-norm，对h求导为例）：</p>
<p><img src="./截屏2022-11-05 10.20.47.png" alt="截屏2022-11-05 10.20.47" style="zoom:80%;" /></p>
<p>若为L1-norm，梯度为[1,-1,-1,…]的形式。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p><img src="./截屏2022-11-05 09.29.04.png" alt="截屏2022-11-05 09.29.04" style="zoom:80%;" /></p>
<ol>
<li>随机初始化实体e和关系l的表示向量，正则化关系l；</li>
<li>每一轮训练都重新将实体e正则化（batch norm）；</li>
<li>随机采样出一批正例，对每一条正例采样一些负例；</li>
<li>用hinge loss更新embedding参数。</li>
</ol>
<blockquote>
<p>Loss更新的参数，是所有entities和relations的Embedding数据，每一次SGD更新的参数就是一个Batch中所有embedding的值</p>
</blockquote>
<p>训练数据集：FB15K，WN18</p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/zqhead/TransE">https://github.com/zqhead/TransE</a></p>
<h2 id="更多"><a href="#更多" class="headerlink" title="更多"></a>更多</h2><p>知识图谱嵌入的Translate模型汇总（TransE，TransH，TransR，TransD）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/147542008">https://zhuanlan.zhihu.com/p/147542008</a></p>
<h1 id="知识图谱相关论文"><a href="#知识图谱相关论文" class="headerlink" title="知识图谱相关论文"></a>知识图谱相关论文</h1><p>19/20年相关论文</p>
<p>商汤/阿里企业的论文</p>
<h1 id="PELT"><a href="#PELT" class="headerlink" title="PELT"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.13392">PELT</a></h1><p><strong>pluggable entity lookup table</strong></p>
<p>为了得到domain-specific的预训练模型，一般需要再训练或者引入外部知识图谱，这里为了不用再训练（耗时耗力），不用引入外部知识图谱，<br>直接在已有经典预训练模型比如bert/roberta上面做些操作（就是直接把所有entity mention对应的output embedding加起来，相对比较简单的一个操作），得到实体嵌入，将传统的词嵌入和现在的实体嵌入一起放进去做表征。</p>
<h1 id="Knowledge-Graph-Embedding-by-Adaptive-Limit-Scoring-Loss-Using-Dynamic-Weighting-Strategy"><a href="#Knowledge-Graph-Embedding-by-Adaptive-Limit-Scoring-Loss-Using-Dynamic-Weighting-Strategy" class="headerlink" title="Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy"></a><a target="_blank" rel="noopener" href="https://aclanthology.org/2022.findings-acl.91.pdf">Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy</a></h1><p>Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links in knowledge graphs. Designing a strong and effective loss framework is essential for knowledge graph embedding models to distinguish between correct and incorrect triplets. The classic margin-based ranking loss limits the scores of positive and negative triplets to have a suitable margin. The recently proposed Limit-based Scoring Loss independently limits the range of positive and negative triplet scores. However, these loss frameworks use equal or fixed penalty terms to reduce the scores of positive and negative sample pairs, which is inflexible in optimization. Our intuition is that if a triplet score deviates far from the optimum, it should be emphasized. To this end, we propose <strong>Adaptive Limit Scoring Loss</strong>, which simply <strong>re-weights each triplet to highlight the less-optimized triplet scores.</strong> We apply this loss framework to several knowledge graph embedding models such as TransE, TransH and ComplEx. The experimental results on link prediction and triplet classification show that our proposed method has achieved performance on par with the state of the art.</p>
<h1 id="Do-Pre-trained-Models-Benefit-Knowledge-Graph-Completion-A-Reliable-Evaluation-and-a-Reasonable-Approach-预训练模型是否有利于KGC？"><a href="#Do-Pre-trained-Models-Benefit-Knowledge-Graph-Completion-A-Reliable-Evaluation-and-a-Reasonable-Approach-预训练模型是否有利于KGC？" class="headerlink" title="Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach 预训练模型是否有利于KGC？"></a><a target="_blank" rel="noopener" href="https://aclanthology.org/2022.findings-acl.282.pdf">Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach</a> 预训练模型是否有利于KGC？</h1><h2 id="补充：KGC任务"><a href="#补充：KGC任务" class="headerlink" title="补充：KGC任务"></a>补充：KGC任务</h2><p>KGC任务旨在补全知识图中缺失的三元组 (h, r, t)。</p>
<p>有两种主要的方法来完成这个任务，即<strong>链接预测和三元组分类</strong></p>
<ul>
<li>前者主要预测三重查询(h,r,?), (h,r,?)或(?,r,t), (?,r,t)</li>
<li>后者的目的是确定给定的三元(h,r,t), (h,r,t)是否正确。</li>
</ul>
<h2 id="补充：KG-BERT-2019"><a href="#补充：KG-BERT-2019" class="headerlink" title="补充：KG-BERT (2019)"></a>补充：KG-BERT (2019)</h2><p>通过语言模型来判断知识图谱三元组是否合理，方法是直接将{h, r, t}拼接为一个句子输入，通过Bert模型来判断合理性</p>
<p><img src="./截屏2022-11-20 10.35.53.png" alt="截屏2022-11-20 10.35.53" style="zoom:67%;" /></p>
<p>或者输入{h, t}判断关系r类型</p>
<p><img src="./截屏2022-11-20 10.37.52.png" alt="截屏2022-11-20 10.37.52" style="zoom:67%;" /></p>
<h2 id="性能不佳的两个原因"><a href="#性能不佳的两个原因" class="headerlink" title="性能不佳的两个原因"></a>性能不佳的两个原因</h2><h3 id="不准确的评估设置"><a href="#不准确的评估设置" class="headerlink" title="不准确的评估设置"></a>不准确的评估设置</h3><p>大多数现有的KGC模型都是在封闭世界假设(Closed-world Assumption, CWA)下进行评估的：在给定的KGs中<strong>未知的任何知识（没有出现的三元组）都是不正确的</strong>。这样的设置有利于自动构建数据集，而无需手动注释。然而，PLMs的引入带来的很多未知的外部知识，都是在CWA下被认为是不正确的，这错误地降低了模型的性能。</p>
<p>开放世界假设 (Open-world assumption, OWA)认为知识图谱中包含的三元组是不完备的。因此，开放世界假设下的评估更准确、更接近真实场景，但需要额外的人工标注，仔细验证知识图谱中不存在的完整三元组是否正确。</p>
<p><img src="./截屏2022-11-20 10.43.37.png" alt="截屏2022-11-20 10.43.37" style="zoom:67%;" /></p>
<h3 id="没有正确利用语言模型"><a href="#没有正确利用语言模型" class="headerlink" title="没有正确利用语言模型"></a>没有正确利用语言模型</h3><p>直接拼接{h, r, t}作为语言模型输入会导致句子不连贯问题</p>
<h2 id="PKGC模型"><a href="#PKGC模型" class="headerlink" title="PKGC模型"></a>PKGC模型</h2><p>一种新的基于PLM的KGC模型，基本思想是<strong>将每个三元组转换成自然的提示句</strong>，而不是简单地拼接它们的标签。具体来说，<strong>为每个关系类型手动定义提示模板</strong>，并进一步引入<strong>软提示</strong>，以更好地表达三元组的语义。此外，得益于提示的调整，PKGC可以灵活地考虑三元组的上下文，例如定义和属性，将它们作为支持提示插入三元组提示的末尾。</p>
<p><img src="./截屏2022-11-20 11.31.13.png" style="zoom:67%"></p>
<blockquote>
<p>Soft prompts: ？文章中没有详细说</p>
</blockquote>
<h2 id="训练任务"><a href="#训练任务" class="headerlink" title="训练任务"></a>训练任务</h2><p><strong>链接预测</strong>和<strong>三元组分类</strong>，前者主要为三元查询(h, r, ?)或(?, r, t)，后者旨在判断给定的三元组(h, r, t)是否正确。</p>
<blockquote>
<p>在三元组分类任务中，需要人工帮助判断负类的三元组（一小部分）是否真的是错误的（CWA假设）</p>
</blockquote>
<h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p>//</p>
<h1 id="Relational-World-Knowledge-Representation-in-Contextual-Language-Models-A-Review-知识图谱综述"><a href="#Relational-World-Knowledge-Representation-in-Contextual-Language-Models-A-Review-知识图谱综述" class="headerlink" title="Relational World Knowledge Representation in Contextual Language Models: A Review 知识图谱综述"></a>Relational World Knowledge Representation in Contextual Language Models: A Review 知识图谱综述</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.05837">https://arxiv.org/abs/2104.05837</a></p>
<p>论文详解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/419803461">https://zhuanlan.zhihu.com/p/419803461</a></p>
<p>基于知识图谱的知识推理</p>
<h1 id="Prompt-learning"><a href="#Prompt-learning" class="headerlink" title="Prompt learning"></a>Prompt learning</h1><p>Prompt learning 相关文章 <a target="_blank" rel="noopener" href="https://github.com/thunlp/PromptPapers">https://github.com/thunlp/PromptPapers</a></p>
<p>open-source prompt-learning toolkit <a target="_blank" rel="noopener" href="https://github.com/thunlp/OpenPrompt">https://github.com/thunlp/OpenPrompt</a></p>
<p>Prompt learning入门 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442486331">https://zhuanlan.zhihu.com/p/442486331</a></p>
<h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><blockquote>
<p>Prompt is the technique of <strong>making better use of the knowledge</strong> from the pre-trained model by <strong>adding additional texts to the input</strong>.</p>
<p>Prompt 是一种<strong>为了更好的使用预训练语言模型的知识</strong>，采用在输入段<strong>添加额外的文本</strong>的技术。</p>
</blockquote>
<p>对于输入的文本x，使用函数$f$，将x转化为prompt形式x’</p>
<p>该函数通常进行两步操作：</p>
<ol>
<li>使用一个模板，模板通常为一段自然语言，并且包含有两个空位置：用于填输入x的位置[X]和用于生成答案文本z的位置[Z].</li>
<li>把输入x填到[X]的位置。</li>
</ol>
<blockquote>
<p>例如：</p>
<p>x = “ I love this movie.”</p>
<p>模版：[X] Overall, it was a [Z] movie.</p>
<p>x’ = I love this movie. Overall it was a [Z] movie.</p>
</blockquote>
<p>接着进行答案搜索，<strong>LM寻找填在[Z]处可以使得分数最高的文本 z 。</strong></p>
<p><strong>最后是答案映射</strong>。</p>
<h2 id="Prompt设计"><a href="#Prompt设计" class="headerlink" title="Prompt设计"></a>Prompt设计</h2><h3 id="Prompt形状"><a href="#Prompt形状" class="headerlink" title="Prompt形状"></a>Prompt形状</h3><p>Prompt的形状主要指的是[X]和[Z]的位置和数量。上文提到<strong>cloze prompt</strong>和<strong>prefix prompt</strong>的区别，在实际应用过程中选择哪一种主要取决于任务的形式和模型的类别。<strong>cloze prompts</strong>和Masked Language Model的训练方式非常类似，因此对于使用MLM的任务来说<strong>cloze prompts</strong>更加合适；对于生成任务来说，或者使用自回归LM解决的任务，<strong>prefix prompts</strong>就会更加合适；Full text reconstruction models较为通用，因此两种prompt均适用。另外，对于文本对的分类，prompt模板通常要给输入预留两个空，[X1]和[X2]。</p>
<h3 id="手工设计模版"><a href="#手工设计模版" class="headerlink" title="手工设计模版"></a>手工设计模版</h3><p>代价太大</p>
<h3 id="自动学习模版"><a href="#自动学习模版" class="headerlink" title="自动学习模版"></a>自动学习模版</h3><p>分为离散（Discrete Prompts）和连续（Continuous Prompts）两大类。离散的主要包括 Prompt Mining, Prompt Paraphrasing, Gradient-based Search, Prompt Generation 和 Prompt Scoring；连续的则主要包括Prefix Tuning, Tuning Initialized with Discrete Prompts 和 Hard-Soft Prompt Hybrid Tuning。</p>
<h4 id="离散Prompts"><a href="#离散Prompts" class="headerlink" title="离散Prompts"></a>离散Prompts</h4><p>自动生成离散Prompts指的是自动生成由自然语言的词组成的Prompt，因此其搜索空间是离散的。目前大致可以分成下面几个方法：</p>
<ol>
<li><strong>Prompt Mining</strong>. 该方法需要一个大的文本库支持，例如Wikipedia。给定输入x和输出y，要找到x和y之间的中间词或者依赖路径，然后选取出现频繁的中间词或依赖路径作为模板，即“[X] middle words [Z]”。</li>
<li><strong>Prompt Paraphrasing</strong>. Paraphrasing-based方法是基于释义的，主要采用现有的种子prompts(例如手动构造)，并将其转述成一组其他候选prompts，然后选择一个在目标任务上达到最好效果的。一般的做法有：将提示符翻译成另一种语言，然后再翻译回来；使用同义或近义短语来替换等。</li>
<li><strong>Gradient-based Search</strong>. 梯度下降搜索的方法是在单词候选集里选择词并组合成prompt，利用梯度下降的方式不断尝试组合，从而达到让PLM生成需要的词的目的。</li>
<li><strong>Prompt Generation</strong>. 既然Prompt也是一段文本，那是否可以用文本生成的方式来生成Prompt呢？该类方法就是将标准的自然语言生成的模型用于生成prompts了。例如，Gao等人将T5引入了模板搜索的过程，让T5生成模板词；Ben-David 等人提出了一种域自适应算法，训练T5为每个输入生成一种唯一的域相关特征，然后把输入和特征连接起来组成模板再用到下游任务中。</li>
<li><strong>Prompt Scoring</strong>. Davison等人在研究知识图谱补全任务的时候为三元组输入（头实体，关系，尾实体）设计了一种模板。首先人工制造一组模板候选，然后把相应的[X]和[Z]都填上成为prompts，并使用一个双向LM给这些prompts打分，最后选取其中的高分prompt。</li>
</ol>
<h4 id="连续Prompts-P-tuning"><a href="#连续Prompts-P-tuning" class="headerlink" title="连续Prompts: P-tuning"></a>连续Prompts: P-tuning</h4><p>直接作用到模型的embedding空间。连续型prompts去掉了两个约束条件：</p>
<ol>
<li>模板中词语的embedding可以是整个自然语言的embedding，不再只是有限的一些embedding。</li>
<li>模板的参数不再直接取PLM的参数，而是有自己独立的参数，可以通过下游任务的训练数据进行调整。</li>
</ol>
<p>方法：</p>
<ol>
<li><strong>Prefix Tuning</strong></li>
<li><strong>Tuing Initialized with Discrete Prompts</strong></li>
<li><strong>Hard-Soft Prompt Hybrid Tuning</strong> 有关P-tuning的方法</li>
</ol>
<h2 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h2><p>P-tuning 自动构建模版《GPT understands, too》 <a target="_blank" rel="noopener" href="https://kexue.fm/archives/8295">https://kexue.fm/archives/8295</a></p>
<p>THUDM P-tuning <a target="_blank" rel="noopener" href="https://github.com/THUDM/P-tuning">https://github.com/THUDM/P-tuning</a></p>
<h1 id="C-3KG-A-Chinese-Commonsense-Conversation-Knowledge-Graph"><a href="#C-3KG-A-Chinese-Commonsense-Conversation-Knowledge-Graph" class="headerlink" title="C^3KG: A Chinese Commonsense Conversation Knowledge Graph"></a><a target="_blank" rel="noopener" href="https://aclanthology.org/2022.findings-acl.107/">C^3KG: A Chinese Commonsense Conversation Knowledge Graph</a></h1><p>构建的中文常识对话知识图谱</p>
<p>4个新的对话流关系：<strong>事件流、概念流、情感-原因流、情感-意图流</strong></p>
<p><img src="./截屏2022-11-22 16.32.55.png" alt="截屏2022-11-22 16.32.55" style="zoom:80%;" /></p>
<p>参考 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/501111403">https://zhuanlan.zhihu.com/p/501111403</a></p>
<h1 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h1><p>生成式预训练模型</p>
<h2 id="与GPT、BERT区别"><a href="#与GPT、BERT区别" class="headerlink" title="与GPT、BERT区别"></a>与GPT、BERT区别</h2><p><img src="./截屏2022-11-27 15.45.48.png" style="zoom:80%"></p>
<ul>
<li>GPT是一种Auto-Regressive(自回归)的语言模型。它也可以看作是Transformer model的Decoder部分，它的优化目标就是标准的语言模型目标：序列中所有token的联合概率。GPT采用的是自然序列中的从左到右（或者从右到左）的因式分解。</li>
<li>BERT是一种Auto-Encoding(自编码)的语言模型。它也可以看作是Transformer model的Encoder部分，在输入端随机使用一种特殊的[MASK]token来替换序列中的token，这也可以看作是一种noise，所以BERT也叫Masked Language Model。</li>
<li>BART吸收了BERT的bidirectional encoder和GPT的left-to-right decoder各自的特点，建立在标准的seq2seq Transformer model的基础之上，这使得它比BERT更适合文本生成的场景；相比GPT，也多了双向上下文语境信息。在生成任务上获得进步的同时，它也可以在一些文本理解类任务上取得SOTA。</li>
<li>训练阶段，Encoder 端使用双向模型编码被破坏的文本，然后 Decoder 采用自回归的方式计算出原始输入；测试阶段或者是微调阶段，Encoder 和 Decoder 的输入都是未被破坏的文本</li>
</ul>
<h2 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h2><p>Loss Function: Reconstruction loss 即decoder的输出和原文ground truth之间的cross entropy</p>
<p>BART的结构在上图中已经很明确了：就是一个BERT+GPT的结构，但是有点不同之处在于（也是作者通篇在强调的），相对于BERT中单一的noise类型(只有简单地用[MASK] token进行替换这一种noise)，BART在encoder端尝试了多种noise。原因是：</p>
<ul>
<li>BERT的这种简单替换导致的是encoder端的输入携带了有关序列结构的一些信息（比如序列的长度等信息），而这些信息在文本生成任务中一般是不会提供给模型的。</li>
<li>BART采用更加多样的noise（更多无监督预训练任务），意图是破坏掉这些有关序列结构的信息，防止模型去“依赖”这样的信息。</li>
</ul>
<p><img src="./截屏2022-11-27 16.13.28.png" style="zoom: 80%"></p>
<h2 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h2><p>得益于auto-regressive的decoder，BART在生成式任务上效果显著。</p>
<h2 id="参考-4"><a href="#参考-4" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/173858031">https://zhuanlan.zhihu.com/p/173858031</a></p>
<p><a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1505/">https://wmathor.com/index.php/archives/1505/</a></p>
<h1 id="Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer"><a href="#Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer" class="headerlink" title="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"></a>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</h1><p>2019 综述文章 Google提出，探索了多种不同的预训练方式，提出了新的数据集和预训练模型T5(Text-to-Text Transfer Transformer)。</p>
<h2 id="“text-to-text”-format"><a href="#“text-to-text”-format" class="headerlink" title="“text-to-text” format"></a>“text-to-text” format</h2><p>一个统一框架，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务。<br><img src="./截屏2022-12-01%2010.58.35.png" style="zoom: 80%"></p>
<p>甚至图片中判断语义相似度的任务也是靠输出浮点数来完成的。</p>
<p>通过这样的方式就能将 NLP 任务都转换成 Text-to-Text 形式，也就可以<strong>用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务</strong>。</p>
<h2 id="无监督任务比较"><a href="#无监督任务比较" class="headerlink" title="无监督任务比较"></a>无监督任务比较</h2><p><img src="。/../截屏2022-12-01%2011.08.47.png" style="zoom: 80%"></p>
<ul>
<li>Prefix LM: 前半部分用于encoder输入，后半部分用于decoder预测</li>
<li>MLM</li>
<li>Deshuffling: 目标是还原整句话</li>
</ul>
<p><img src="./截屏2022-12-01%2011.13.16.png" style="zoom: 80%"></p>
<p>发现还是BERT-Style比较好，随后考虑了BERT-Style的变种</p>
<ul>
<li>MASS-Style: 15%的mask全部替换为[mask]</li>
<li>Replace spans: 替换为mask的范围序列用一个[span-mask]来代替，baseline方法中使用</li>
<li>Drop tokens: 直接随机删除一些词，然后让模型预测删除的词？</li>
</ul>
<p>表现都差不多，Replace corrupted spans略好，不需要预测完整序列的方法更好，因为让target更短了。<br><img src="./截屏2022-12-01%2011.22.09.png" style="zoom: 80%"></p>
<p>Corruption rate对比：还是15%最好</p>
<p><img src="./截屏2022-12-01%2011.31.27.png" style="zoom: 80%"><br>Corruption span长度对比：区别有限，但更高的span长度会加速训练，因为预测target数量更少了。（这里的 [i.i.d.] 是BERT预训练任务中的mask token方法）。在处理非翻译任务上使用 span=3 比 i.i.d. 好一丢丢。</p>
<h2 id="T5预训练模型"><a href="#T5预训练模型" class="headerlink" title="T5预训练模型"></a>T5预训练模型</h2><ul>
<li>Transformer encoder-decoder模型，用于text-to-text</li>
<li>Mask方法</li>
<li>Replace corrupted spans目标</li>
<li>15%破坏</li>
<li>span=3</li>
</ul>
<h2 id="预训练数据集"><a href="#预训练数据集" class="headerlink" title="预训练数据集"></a>预训练数据集</h2><p>C4</p>
<h2 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h2><p>微调模型所有的参数会导致非最优的结果，这里尝试两种方法，只更新模型的一小部分参数。</p>
<h3 id="adapter-layers"><a href="#adapter-layers" class="headerlink" title="adapter layers"></a>adapter layers</h3><p>在之前 Transformer 的基础上，在每个 block 的每个 feed-forward 层之后加上全新的 dense-ReLU-dense 层。我们只需微调这个层和 layer norm。第二个 dense 层维度被设置为匹配后面的输入，第一个 dense 层的维度考虑赋为不同的值以观察效果。</p>
<blockquote>
<p>Adapter layers are additional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward networks in each block of the Transformer.</p>
</blockquote>
<p>只要将维度适当地缩放到任务大小，adapter layers 可能是一种在较少参数量上有前途的微调方案。</p>
<h3 id="gradual-freezing"><a href="#gradual-freezing" class="headerlink" title="gradual freezing"></a>gradual freezing</h3><p>随着时间的流逝，从最后一层到所有的层，越来越多的模型参数会进行微调。在 encoder-decoder 模型中，从尾到头并行的解冻 encoder 和 decoder 中的层。而输出分类矩阵和输入嵌入矩阵共享，所以整个过程它们都参与微调</p>
<blockquote>
<p>at the start of fine-tuning only the parameters of the final layer are updated, then after training for a certain number of updates the parameters of the second-to-last layer are also included, and so on until the entire network’s parameters are being fine-tuned.</p>
<p>gradually unfreeze layers in the encoder and decoder in parallel, starting from the top in both cases.</p>
</blockquote>
<p>效果一般</p>
<h2 id="Multi-task-Learning"><a href="#Multi-task-Learning" class="headerlink" title="Multi-task Learning"></a>Multi-task Learning</h2><p>在预训练阶段对于该模型同时训练多个任务，也即该模型参数在这些任务间共享。但这里只是简单的混合所有预训练数据集</p>
<ul>
<li>Example-proportional mixing 人为限制不同任务数据集的最大数量 probability of sampling an example from the mth task during training to $r_m = min(e_m, K)/ \sum min(e_n, K)$ where $K$ is the artificial data set size limit.</li>
<li>Temperature-sclaed mixing 对每个 $r_m$ 都执行 $1/T$ 次幂，再把它们重新归一化为和为 1 的采样概率。</li>
<li>Equal mixing 每个数据集采样概率相等</li>
</ul>
<p>multi-task learning 不如 预训练-微调 的方式</p>
<h2 id="Multi-task-Learning与微调结合"><a href="#Multi-task-Learning与微调结合" class="headerlink" title="Multi-task Learning与微调结合"></a>Multi-task Learning与微调结合</h2><p>Multi-task pretraining + fine-tuning 效果与 Unsupervised pre-training + fine-tuning 相当！</p>
<h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><p>如果拥有了 4 倍的计算量，应该如何去使用它？</p>
<h2 id="T5-Text-to-Text-Transfer-Transformer"><a href="#T5-Text-to-Text-Transfer-Transformer" class="headerlink" title="T5 (Text-to-Text Transfer Transformer)"></a>T5 (Text-to-Text Transfer Transformer)</h2><ul>
<li>Objective 使用平均 span 长度为 3 ，掩码率为 15％ 的 span corruption 目标。</li>
<li>Longer training 使用 C4 数据集预训练 1 million 步， batch size，句子长度为 512，总共大约 1 trillion tokens。</li>
<li><p>Model size</p>
<p>  |       | $d_{model}$ | $d_{ff}$ | $d_{kv}$ | head | layer | params      |<br>  |———-|——————-|—————|—————|———|———-|——————-|<br>  | Small | 512         | 2048     | 64       | 8    | 6     | 60 million  |<br>  | Base  | 768         | 3072     | 64       | 12   | 12    | 220 million |<br>  | Large | 1024        | 4096     | 64       | 16   | 12    | 770 million |<br>  | 3B    | 1024        | 16384    | 128      | 32   | 24    | 2.8 billion |<br>  | 11B   | 1024        | 65536    | 128      | 128  | 24    | 11 billion  |</p>
</li>
<li><p>Multi-task pre-training 对每个大小的模型使用不同大小的数据集，再使用 example-proportional mixing 得到多任务的数据集，并对于翻译任务的数据集大小做了适当限制。</p>
</li>
<li>Fine-tuning on individual GLUE and SuperGLUE tasks 对 benchmark 的各个任务单独 fine-tune 可以小幅提升模型性能。为了不在某些 low-resource 任务上过拟合，在 fine-tuning 阶段选择更小的 batch size 为 8，每 1000 步做一次验证。也执行了统一的一次性微调。最终对于每个人物选择统一微调和单独微调中验证效果最好的结果。</li>
<li>Beam search 对于长输出的句子，使用 beam search 可以提升效果。</li>
<li>Test set 使用标准的 test set 报道结果（除 SQuAD 之外）</li>
</ul>
<p>参考：<br><a target="_blank" rel="noopener" href="https://suixinblog.cn/2020/04/t5.html#fn_2">https://suixinblog.cn/2020/04/t5.html#fn_2</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/88438851">https://zhuanlan.zhihu.com/p/88438851</a>  </p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag"># 学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/26/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/NLP-basic/" rel="prev" title="NLP_basic">
      <i class="fa fa-chevron-left"></i> NLP_basic
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Learning-for-Sentiment-Analysis-A-Survey"><span class="nav-number">1.</span> <span class="nav-text">Deep Learning for Sentiment Analysis: A Survey</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95"><span class="nav-number">1.2.</span> <span class="nav-text">传统方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Word-Embedding"><span class="nav-number">1.3.</span> <span class="nav-text">Word Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">深度学习方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sentiment-Analysis-Tasks"><span class="nav-number">1.5.</span> <span class="nav-text">Sentiment Analysis Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Document-Level"><span class="nav-number">1.5.1.</span> <span class="nav-text">Document Level</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sentence-Level"><span class="nav-number">1.5.2.</span> <span class="nav-text">Sentence Level</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Aspect-Level"><span class="nav-number">1.5.3.</span> <span class="nav-text">Aspect Level</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Aspect-extraction-amp-categorization"><span class="nav-number">1.6.</span> <span class="nav-text">Aspect extraction &amp; categorization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Opinion-expression-extraction"><span class="nav-number">1.7.</span> <span class="nav-text">Opinion expression extraction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sentiment-composition"><span class="nav-number">1.8.</span> <span class="nav-text">Sentiment composition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Opinion-holder-extraction"><span class="nav-number">1.9.</span> <span class="nav-text">Opinion holder extraction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Temporal-opnion-mining"><span class="nav-number">1.10.</span> <span class="nav-text">Temporal opnion mining</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sentiment-analysis-with-word-embedding"><span class="nav-number">1.11.</span> <span class="nav-text">Sentiment analysis with word embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sarcasm-analysis"><span class="nav-number">1.12.</span> <span class="nav-text">Sarcasm analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Emotion-analysis"><span class="nav-number">1.13.</span> <span class="nav-text">Emotion analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multimodal-data-for-sentiment-analysis"><span class="nav-number">1.14.</span> <span class="nav-text">Multimodal data for sentiment analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Resource-poor-languages-and-multilingual-sentiment-analysis"><span class="nav-number">1.15.</span> <span class="nav-text">Resource-poor languages and multilingual sentiment analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">1.16.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RoBERTa"><span class="nav-number">2.</span> <span class="nav-text">RoBERTa</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ELECTRA"><span class="nav-number">3.</span> <span class="nav-text">ELECTRA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E6%96%B0"><span class="nav-number">3.1.</span> <span class="nav-text">创新</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#XLNet"><span class="nav-number">4.</span> <span class="nav-text">XLNet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AR-or-AE"><span class="nav-number">4.1.</span> <span class="nav-text">AR or AE ?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Permutation-language-model"><span class="nav-number">4.2.</span> <span class="nav-text">Permutation language model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95Permute%EF%BC%9F"><span class="nav-number">4.2.1.</span> <span class="nav-text">如何Permute？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%EF%BC%9F"><span class="nav-number">4.2.2.</span> <span class="nav-text">位置信息？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Two-Stream-Self-Attention"><span class="nav-number">4.2.3.</span> <span class="nav-text">Two-Stream Self-Attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-XL"><span class="nav-number">4.3.</span> <span class="nav-text">Transformer-XL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Segment-Recurrence-Mechanism-%E6%AE%B5%E5%BE%AA%E7%8E%AF%E6%9C%BA%E5%88%B6"><span class="nav-number">4.3.1.</span> <span class="nav-text">Segment Recurrence Mechanism 段循环机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Relative-Position-Encoding-%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">4.3.2.</span> <span class="nav-text">Relative Position Encoding 相对位置编码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Relative-Segment-Encoding-%E7%9B%B8%E5%AF%B9%E6%AE%B5%E7%BC%96%E7%A0%81%EF%BC%9F"><span class="nav-number">4.4.</span> <span class="nav-text">Relative Segment Encoding 相对段编码？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XLNet-%E4%B8%8E-Bert-%E5%AF%B9%E6%AF%94"><span class="nav-number">4.5.</span> <span class="nav-text">XLNet 与 Bert 对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86"><span class="nav-number">4.6.</span> <span class="nav-text">代码部分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#data-utils-py"><span class="nav-number">4.6.1.</span> <span class="nav-text">data_utils.py</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-1"><span class="nav-number">4.7.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BERT-CHN-WWM"><span class="nav-number">5.</span> <span class="nav-text">BERT-CHN-WWM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#wwm-whole-word-masking-or-N-gram-mask"><span class="nav-number">5.1.</span> <span class="nav-text">wwm(whole word masking) or N-gram mask?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="nav-number">5.2.</span> <span class="nav-text">预训练任务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mac-MLM-as-correction"><span class="nav-number">5.2.1.</span> <span class="nav-text">Mac(MLM as correction)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SOP-sentence-order-prediction"><span class="nav-number">5.2.2.</span> <span class="nav-text">SOP(sentence order prediction)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ERNIE-THU"><span class="nav-number">6.</span> <span class="nav-text">ERNIE-THU</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Detail"><span class="nav-number">6.1.</span> <span class="nav-text">Detail</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#information-fusion"><span class="nav-number">6.1.1.</span> <span class="nav-text">information fusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TransE"><span class="nav-number">6.2.</span> <span class="nav-text">TransE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F-dEA"><span class="nav-number">6.3.</span> <span class="nav-text">预训练方式 dEA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fine-tuning-for-specific-tasks"><span class="nav-number">6.4.</span> <span class="nav-text">Fine-tuning for specific tasks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GLUE-Result"><span class="nav-number">6.5.</span> <span class="nav-text">GLUE Result</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ablation-study"><span class="nav-number">6.6.</span> <span class="nav-text">Ablation study</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-2"><span class="nav-number">6.7.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ERNIE-Baidu-1-0"><span class="nav-number">7.</span> <span class="nav-text">ERNIE-Baidu 1.0</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Knowledge-Masking"><span class="nav-number">7.1.</span> <span class="nav-text">Knowledge Masking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9B%B4%E5%A4%9A%E7%9A%84%E8%AF%AD%E6%96%99%E4%BF%A1%E6%81%AF"><span class="nav-number">7.2.</span> <span class="nav-text">更多的语料信息</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ERNIE-Baidu-2-0"><span class="nav-number">8.</span> <span class="nav-text">ERNIE-Baidu 2.0</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Continual-Pre-training"><span class="nav-number">8.1.</span> <span class="nav-text">Continual Pre-training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pre-training-tasks"><span class="nav-number">8.2.</span> <span class="nav-text">Pre-training tasks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Continual-Fine-tuning"><span class="nav-number">8.3.</span> <span class="nav-text">Continual Fine-tuning</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Trans%E5%AE%B6%E6%97%8F%E6%A8%A1%E5%9E%8B"><span class="nav-number">9.</span> <span class="nav-text">Trans家族模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TransE-1"><span class="nav-number">9.1.</span> <span class="nav-text">TransE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">9.1.1.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">9.1.2.</span> <span class="nav-text">训练过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9B%B4%E5%A4%9A"><span class="nav-number">9.2.</span> <span class="nav-text">更多</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="nav-number">10.</span> <span class="nav-text">知识图谱相关论文</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PELT"><span class="nav-number">11.</span> <span class="nav-text">PELT</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Knowledge-Graph-Embedding-by-Adaptive-Limit-Scoring-Loss-Using-Dynamic-Weighting-Strategy"><span class="nav-number">12.</span> <span class="nav-text">Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Do-Pre-trained-Models-Benefit-Knowledge-Graph-Completion-A-Reliable-Evaluation-and-a-Reasonable-Approach-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E6%9C%89%E5%88%A9%E4%BA%8EKGC%EF%BC%9F"><span class="nav-number">13.</span> <span class="nav-text">Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach 预训练模型是否有利于KGC？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%A5%E5%85%85%EF%BC%9AKGC%E4%BB%BB%E5%8A%A1"><span class="nav-number">13.1.</span> <span class="nav-text">补充：KGC任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A1%A5%E5%85%85%EF%BC%9AKG-BERT-2019"><span class="nav-number">13.2.</span> <span class="nav-text">补充：KG-BERT (2019)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E4%B8%8D%E4%BD%B3%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%8E%9F%E5%9B%A0"><span class="nav-number">13.3.</span> <span class="nav-text">性能不佳的两个原因</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%87%86%E7%A1%AE%E7%9A%84%E8%AF%84%E4%BC%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">13.3.1.</span> <span class="nav-text">不准确的评估设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B2%A1%E6%9C%89%E6%AD%A3%E7%A1%AE%E5%88%A9%E7%94%A8%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">13.3.2.</span> <span class="nav-text">没有正确利用语言模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PKGC%E6%A8%A1%E5%9E%8B"><span class="nav-number">13.4.</span> <span class="nav-text">PKGC模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="nav-number">13.5.</span> <span class="nav-text">训练任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Analysis"><span class="nav-number">13.6.</span> <span class="nav-text">Analysis</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Relational-World-Knowledge-Representation-in-Contextual-Language-Models-A-Review-%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0"><span class="nav-number">14.</span> <span class="nav-text">Relational World Knowledge Representation in Contextual Language Models: A Review 知识图谱综述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Prompt-learning"><span class="nav-number">15.</span> <span class="nav-text">Prompt learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic"><span class="nav-number">15.1.</span> <span class="nav-text">Basic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prompt%E8%AE%BE%E8%AE%A1"><span class="nav-number">15.2.</span> <span class="nav-text">Prompt设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prompt%E5%BD%A2%E7%8A%B6"><span class="nav-number">15.2.1.</span> <span class="nav-text">Prompt形状</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%8B%E5%B7%A5%E8%AE%BE%E8%AE%A1%E6%A8%A1%E7%89%88"><span class="nav-number">15.2.2.</span> <span class="nav-text">手工设计模版</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E7%89%88"><span class="nav-number">15.2.3.</span> <span class="nav-text">自动学习模版</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3Prompts"><span class="nav-number">15.2.3.1.</span> <span class="nav-text">离散Prompts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E7%BB%ADPrompts-P-tuning"><span class="nav-number">15.2.3.2.</span> <span class="nav-text">连续Prompts: P-tuning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-3"><span class="nav-number">15.3.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#C-3KG-A-Chinese-Commonsense-Conversation-Knowledge-Graph"><span class="nav-number">16.</span> <span class="nav-text">C^3KG: A Chinese Commonsense Conversation Knowledge Graph</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BART"><span class="nav-number">17.</span> <span class="nav-text">BART</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8EGPT%E3%80%81BERT%E5%8C%BA%E5%88%AB"><span class="nav-number">17.1.</span> <span class="nav-text">与GPT、BERT区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%86%E8%8A%82"><span class="nav-number">17.2.</span> <span class="nav-text">模型细节</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1"><span class="nav-number">17.3.</span> <span class="nav-text">下游任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-4"><span class="nav-number">17.4.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer"><span class="nav-number">18.</span> <span class="nav-text">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%80%9Ctext-to-text%E2%80%9D-format"><span class="nav-number">18.1.</span> <span class="nav-text">“text-to-text” format</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E4%BB%BB%E5%8A%A1%E6%AF%94%E8%BE%83"><span class="nav-number">18.2.</span> <span class="nav-text">无监督任务比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#T5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">18.3.</span> <span class="nav-text">T5预训练模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">18.4.</span> <span class="nav-text">预训练数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="nav-number">18.5.</span> <span class="nav-text">训练方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adapter-layers"><span class="nav-number">18.5.1.</span> <span class="nav-text">adapter layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradual-freezing"><span class="nav-number">18.5.2.</span> <span class="nav-text">gradual freezing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-task-Learning"><span class="nav-number">18.6.</span> <span class="nav-text">Multi-task Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-task-Learning%E4%B8%8E%E5%BE%AE%E8%B0%83%E7%BB%93%E5%90%88"><span class="nav-number">18.7.</span> <span class="nav-text">Multi-task Learning与微调结合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scaling"><span class="nav-number">18.8.</span> <span class="nav-text">Scaling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#T5-Text-to-Text-Transfer-Transformer"><span class="nav-number">18.9.</span> <span class="nav-text">T5 (Text-to-Text Transfer Transformer)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="chaofan"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">chaofan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hcffffff" title="Github → https:&#x2F;&#x2F;github.com&#x2F;hcffffff" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>Github</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hechaofan1@outlook.com" title="Email → mailto:hechaofan1@outlook.com" rel="noopener" target="_blank"><i class="far fa-envelope-open fa-fw"></i>Email</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chaofan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
